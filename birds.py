# -*- coding: utf-8 -*-
"""Untitled22.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d3SEIOBDvkpRGnvFPoUpdnIjthYMHpFe
"""

from google.colab import drive
drive.mount('/content/drive')

from google.colab import drive
drive.mount('/content/gdrive')

import zipfile

dataset_path="/content/gdrive/My Drive/534640_1132850_bundle_archive.zip"
zfile=zipfile.ZipFile(dataset_path)
zfile.extractall()

import numpy as np
import keras
from keras.models import Sequential
from keras.layers.core import Dense, Flatten, Dropout
from keras.preprocessing.image import ImageDataGenerator
from keras.preprocessing import image
import shutil
import os
import cv2

from keras.applications.vgg16 import VGG16, preprocess_input

train_path = 'train'       
test_path = 'test'           
validate_path = 'valid'


train_datagen = ImageDataGenerator(
        preprocessing_function=preprocess_input,
        shear_range=0.1, 
        zoom_range=0.1,
        horizontal_flip=True)
train_generator = train_datagen.flow_from_directory(
        'train',
        target_size=(224, 224),
        batch_size=64,
        class_mode='categorical')


val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
val_generator = val_datagen.flow_from_directory(
        'valid',
        target_size=(224, 224),
        batch_size=32,
        class_mode='categorical')


test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
test_generator = test_datagen.flow_from_directory(
        'test',
        target_size=(224, 224),
        batch_size=32,
        class_mode='categorical')


base_model=keras.applications.VGG16(
    include_top=False,
    weights="imagenet",
    input_shape=(224,224,3))


base_model.trainable = False


from keras.models import Sequential
from keras.layers import Dense,Flatten,Dropout
model=Sequential()
model.add(base_model)
model.add(Flatten())
model.add(Dense(2048,activation='relu',kernel_initializer='he_normal'))
model.add(Dropout(0.35))
model.add(Dense(2048,activation='relu',kernel_initializer='he_normal'))
model.add(Dropout(0.35))
model.add(Dense(200,activation='softmax',kernel_initializer='glorot_normal'))

model.summary()

#Train the model on new data.
model.compile(optimizer=keras.optimizers.Adam(1e-4),loss='categorical_crossentropy',metrics=['accuracy'])
history=model.fit(train_generator,epochs=50,validation_data=val_generator,workers=10,use_multiprocessing=True)

import matplotlib.pyplot as plt
#Loss
plt.plot(history.history['loss'],label='loss')
plt.plot(history.history['val_loss'],label='val_loss')
plt.legend()
plt.show()

plt.plot(history.history['accuracy'],label='acc')
plt.plot(history.history['val_accuracy'],label='val_acc')
plt.legend()
plt.show()

model.save("model.h5")

model.evaluate(test_generator)

import matplotlib.pyplot as plt

test_generator.reset()

pred=model.predict_generator(test_generator,verbose=1)

predicted_class_indices=np.argmax(pred,axis=1)

labels = (train_generator.class_indices)
labels = dict((v,k) for k,v in labels.items())
predictions = [labels[k] for k in predicted_class_indices]

print(labels)

x,y = test_generator.next()
label = np.argmax(y,axis=1)
predictedvalues = model.predict(x)
bb = np.argmax(predictedvalues,axis=1)
b = np.argmax(y,axis=1)
m = []
for i in range(32):
    image = x[i]
    plt.imshow(image/255)
    plt.show()
    print("Test Label is",labels[b[i]])
    print("Model predicted is",labels[bb[i]])
    if b[i]!=bb[i]:
      m.append(i)

if len(m) != 0:
  for i in m:
      image = x[i]
      plt.imshow(image/255)
      plt.show()
      print("Test Label is",labels[b[i]])
      print("Model predicted is",labels[bb[i]])
      print("It is a mismatch")

base_model=model.layers[0]

base_model.trainable = True

set_trainable = False
for layer in base_model.layers:
    if layer.name == 'block4_conv1':
        set_trainable = True
    if set_trainable:
        layer.trainable = True
    else:
        layer.trainable = False
        
base_model.summary()
model.summary()

model.compile(optimizer=keras.optimizers.Adam(1e-5),loss='categorical_crossentropy',metrics=['accuracy'])

history=model.fit(train_generator,epochs=30,validation_data=val_generator,workers=10,use_multiprocessing=True)

model.compile(optimizer=keras.optimizers.Adam(1e-6),loss='categorical_crossentropy',metrics=['accuracy'])
history=model.fit(train_generator,epochs=10,validation_data=val_generator,workers=10,use_multiprocessing=True)

model.save("model_fine_tuned")

model.evaluate(test_generator)